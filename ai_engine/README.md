# QBITEL Bridge - AI Engine

## Enterprise-Grade Protocol Discovery, Field Detection, and Anomaly Detection

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-enabled-blue.svg)](https://docker.com)
[![Kubernetes](https://img.shields.io/badge/kubernetes-ready-blue.svg)](https://kubernetes.io)
[![QBITEL Bridge](https://img.shields.io/badge/QBITEL-Bridge-purple.svg)](https://qbitel.com/bridge)

The QBITEL Bridge AI Engine is a production-ready, enterprise-grade artificial intelligence system designed for automated protocol discovery, intelligent field detection, and real-time anomaly detection in network protocols and data streams. This is the core AI component of [QBITEL Bridge](https://qbitel.com/bridge).

## ğŸš€ Features

### Core AI Capabilities
- **Protocol Discovery**: Advanced PCFG (Probabilistic Context-Free Grammar) inference for automatic protocol learning
- **Field Detection**: BiLSTM-CRF models for precise field boundary detection with IOB tagging
- **Anomaly Detection**: Variational Autoencoder (VAE) based system for detecting protocol anomalies
- **Feature Engineering**: Comprehensive pipeline with statistical, structural, and contextual features
- **Ensemble Models**: Advanced ensemble methods for improved prediction accuracy

### Enterprise Architecture
- **Scalable Design**: Distributed processing with async/await patterns
- **Production Ready**: Comprehensive error handling, logging, and monitoring
- **Model Management**: Advanced MLflow integration with model versioning and registry
- **API Interfaces**: Both REST (FastAPI) and gRPC APIs for maximum compatibility
- **Security**: Authentication, authorization, rate limiting, and data validation
- **Observability**: Distributed tracing, metrics collection, and health monitoring

### Deployment Options
- **Docker**: Production-ready containers with multi-stage builds
- **Kubernetes**: Comprehensive deployment manifests with RBAC and monitoring
- **Cloud Native**: Designed for cloud deployment with auto-scaling capabilities
- **Development**: Docker Compose setup with all supporting services

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Architecture](#architecture)
- [API Documentation](#api-documentation)
- [Configuration](#configuration)
- [Deployment](#deployment)
- [Monitoring](#monitoring)
- [Development](#development)
- [Testing](#testing)
- [Contributing](#contributing)
- [License](#license)

## ğŸ”§ Installation

### Prerequisites

- Python 3.10+
- Docker (for containerized deployment)
- Kubernetes cluster (for production deployment)
- CUDA-compatible GPU (optional, for accelerated training)

### Local Development Setup

```bash
# Clone the repository
git clone https://github.com/yazhsab/ai-engine.git
cd ai-engine

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install AI Engine in development mode
pip install -e .

# Initialize the system
python -m ai_engine.tests setup_test_environment
```

### Docker Installation

```bash
# Build the Docker image
docker build -f deployment/docker/Dockerfile -t qbitel/ai-engine:latest .

# Run with Docker Compose (recommended for development)
cd deployment/docker
docker-compose up -d
```

## ğŸš€ Quick Start

### Running the AI Engine

```bash
# Start the AI Engine with default configuration
python -m ai_engine

# Start with custom configuration
python -m ai_engine --config config/production.yml --log-level INFO

# Start with both REST and gRPC APIs
python -m ai_engine --enable-grpc --port 8000 --grpc-port 50051

# Development mode with hot reloading
python -m ai_engine --development --reload
```

### Basic Usage Examples

#### Protocol Discovery

```python
import asyncio
from ai_engine.core.engine import AIEngine
from ai_engine.core.config import Config
from ai_engine.models import ModelInput

async def discover_protocol():
    # Initialize AI Engine
    config = Config()
    engine = AIEngine(config)
    await engine.initialize()
    
    # Protocol data to analyze
    protocol_data = b"GET /api/v1/users HTTP/1.1\r\nHost: example.com\r\n\r\n"
    
    # Create input
    model_input = ModelInput(data=protocol_data)
    
    # Discover protocol
    result = await engine.discover_protocol(model_input)
    
    print(f"Discovered Protocol: {result.metadata['protocol_type']}")
    print(f"Confidence: {result.metadata['confidence']:.2f}")
    
    await engine.cleanup()

# Run the example
asyncio.run(discover_protocol())
```

#### REST API Usage

```bash
# Health check
curl http://localhost:8000/health

# Protocol discovery
curl -X POST http://localhost:8000/discovery/protocols \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "data": "R0VUIC9hcGkvdjEvdXNlcnMgSFRUUC8xLjENCkhvc3Q6IGV4YW1wbGUuY29tDQoNCg==",
    "data_format": "base64",
    "confidence_threshold": 0.7
  }'

# Field detection
curl -X POST http://localhost:8000/detection/fields \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "data": "R0VUIC9hcGkvdjEvdXNlcnMgSFRUUC8xLjENCkhvc3Q6IGV4YW1wbGUuY29tDQoNCg==",
    "data_format": "base64",
    "protocol_hint": "http"
  }'

# Anomaly detection
curl -X POST http://localhost:8000/detection/anomalies \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -d '{
    "data": "R0VUIC9hcGkvdjEvdXNlcnMgSFRUUC8xLjENCkhvc3Q6IGV4YW1wbGUuY29tDQoNCg==",
    "data_format": "base64",
    "sensitivity": "medium"
  }'
```

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        QBITEL Engine                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  APIs (REST/gRPC)     â”‚  Core Engine    â”‚  Monitoring/Observ.  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ FastAPI         â”‚  â”‚  â”‚ AI Engine   â”‚ â”‚  â”‚ Prometheus      â”‚  â”‚
â”‚  â”‚ gRPC Server     â”‚  â”‚  â”‚ Orchestratorâ”‚ â”‚  â”‚ Distributed     â”‚  â”‚
â”‚  â”‚ Authentication  â”‚  â”‚  â”‚             â”‚ â”‚  â”‚ Tracing         â”‚  â”‚
â”‚  â”‚ Rate Limiting   â”‚  â”‚  â”‚             â”‚ â”‚  â”‚ Health Checks   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    AI Components                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Protocol        â”‚  â”‚ Field Detection â”‚  â”‚ Anomaly         â”‚  â”‚
â”‚  â”‚ Discovery       â”‚  â”‚                 â”‚  â”‚ Detection       â”‚  â”‚
â”‚  â”‚ (PCFG Inference)â”‚  â”‚ (BiLSTM-CRF)    â”‚  â”‚ (VAE)           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                 Supporting Infrastructure                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Feature         â”‚  â”‚ Model Registry  â”‚  â”‚ Training        â”‚  â”‚
â”‚  â”‚ Engineering     â”‚  â”‚ & Versioning    â”‚  â”‚ Infrastructure  â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚ (MLflow)        â”‚  â”‚                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Component Details

#### Core AI Components

1. **Protocol Discovery Engine** (`ai_engine/discovery/`)
   - PCFG (Probabilistic Context-Free Grammar) inference
   - Statistical feature extraction
   - Protocol classification and confidence scoring

2. **Field Detection System** (`ai_engine/detection/`)
   - BiLSTM-CRF neural networks for sequence labeling
   - IOB (Inside-Outside-Begin) tagging scheme
   - Field boundary detection and semantic classification

3. **Anomaly Detection System** (`ai_engine/anomaly/`)
   - Variational Autoencoder (VAE) architecture
   - Reconstruction error analysis
   - Statistical and sequential anomaly detection

4. **Feature Engineering** (`ai_engine/features/`)
   - Statistical feature extraction (entropy, frequency, patterns)
   - Structural analysis (length distributions, patterns)
   - Contextual features (n-grams, positional encoding)

5. **Model Management** (`ai_engine/models/`)
   - Base model interfaces and validation
   - Model registry with versioning
   - Ensemble methods and meta-learning

#### Infrastructure Components

6. **Training Infrastructure** (`ai_engine/training/`)
   - MLflow integration for experiment tracking
   - Distributed training support
   - Hyperparameter optimization with Optuna

7. **API Layer** (`ai_engine/api/`)
   - FastAPI-based REST API with OpenAPI documentation
   - gRPC services for high-performance communication
   - Authentication, authorization, and rate limiting

8. **Monitoring & Observability** (`ai_engine/monitoring/`)
   - Prometheus metrics collection
   - Distributed tracing with custom implementation
   - Health monitoring and alerting system
   - Structured logging with contextual information

## ğŸ“š API Documentation

### REST API Endpoints

The AI Engine provides comprehensive REST API endpoints:

- **Health & Status**
  - `GET /health` - Health check
  - `GET /status` - Detailed system status

- **Protocol Discovery**
  - `POST /discovery/protocols` - Discover protocol from data

- **Field Detection**
  - `POST /detection/fields` - Detect fields in protocol data

- **Anomaly Detection**
  - `POST /detection/anomalies` - Detect anomalies in data

- **Model Management**
  - `GET /models` - List registered models
  - `POST /models` - Register new model

- **Batch Processing**
  - `POST /batch` - Process multiple items in batch
  - `GET /batch/{batch_id}` - Get batch status

### API Documentation Access

When running the AI Engine, interactive API documentation is available at:
- Swagger UI: `http://localhost:8000/docs`
- ReDoc: `http://localhost:8000/redoc`
- OpenAPI JSON: `http://localhost:8000/openapi.json`

### gRPC API

The gRPC API provides high-performance communication for:
- Protocol discovery
- Field detection
- Anomaly detection
- Batch processing
- Health checks

## âš™ï¸ Configuration

### Configuration Files

The AI Engine uses YAML configuration files:

```yaml
# config/production.yml
# Server Configuration
rest_host: "0.0.0.0"
rest_port: 8000
grpc_port: 50051

# AI Configuration
model_path: "/app/models"
data_path: "/app/data"
device: "cpu"  # or "cuda" for GPU
batch_size: 32

# MLflow Configuration
mlflow:
  tracking_uri: "http://mlflow:5000"
  experiment_name: "qbitel_production"

# Monitoring Configuration
metrics_port: 9090
health_check_interval: 30
enable_observability: true

# Logging Configuration
log_level: "INFO"
log_format: "structured"
enable_tracing: true
```

### Environment Variables

```bash
# Core Configuration
AI_ENGINE_CONFIG_PATH=/app/config/production.yml
AI_ENGINE_LOG_LEVEL=INFO
AI_ENGINE_DATA_PATH=/app/data
AI_ENGINE_MODEL_PATH=/app/models

# MLflow Integration
MLFLOW_TRACKING_URI=http://mlflow:5000

# Monitoring
PROMETHEUS_METRICS_PORT=9090
```

## ğŸš€ Deployment

### Docker Deployment

```bash
# Build and run with Docker Compose
cd deployment/docker
docker-compose up -d

# Check status
docker-compose ps
docker-compose logs qbitel-engine
```

### Kubernetes Deployment

```bash
# Deploy to Kubernetes cluster
kubectl apply -f deployment/kubernetes/namespace.yaml
kubectl apply -f deployment/kubernetes/deployment.yaml

# Check deployment status
kubectl get pods -n qbitel
kubectl get services -n qbitel

# Access logs
kubectl logs -n qbitel deployment/qbitel-engine -f
```

### Production Considerations

1. **Resource Requirements**
   - CPU: 2+ cores recommended
   - Memory: 4GB+ RAM
   - Storage: 50GB+ for models and data
   - GPU: Optional but recommended for training

2. **Security**
   - Enable authentication and authorization
   - Use TLS/SSL for external communication
   - Configure proper RBAC in Kubernetes
   - Secure API keys and secrets

3. **Scalability**
   - Configure horizontal pod autoscaling
   - Use persistent volumes for data and models
   - Set up load balancing for multiple instances
   - Monitor resource usage and performance

## ğŸ“Š Monitoring

### Metrics Collection

The AI Engine exposes Prometheus metrics:

```bash
# Access metrics endpoint
curl http://localhost:9090/metrics

# Key metrics include:
# - qbitel_protocol_discovery_requests_total
# - qbitel_field_detection_duration_seconds
# - qbitel_anomaly_detection_requests_total
# - qbitel_model_inference_duration_seconds
# - qbitel_system_cpu_percent
# - qbitel_system_memory_percent
```

### Dashboards

Grafana dashboards are included for monitoring:
- System metrics (CPU, memory, disk)
- Application metrics (requests, errors, latency)
- AI-specific metrics (model performance, accuracy)
- Business metrics (protocols discovered, anomalies detected)

Access Grafana at `http://localhost:3000` (admin/qbitel-admin)

### Health Monitoring

Comprehensive health checks include:
- System resources (CPU, memory, disk)
- AI Engine components status
- Model availability and performance
- External dependencies (MLflow, databases)

## ğŸ”§ Development

### Development Setup

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run in development mode
python -m ai_engine --development --reload

# Run tests
python -m pytest tests/ -v

# Run specific test categories
python -m pytest tests/test_core.py -v
python -m pytest tests/test_api.py -v

# Generate coverage report
python -m pytest tests/ --cov=ai_engine --cov-report=html
```

### Code Structure

```
ai_engine/
â”œâ”€â”€ core/           # Core engine and configuration
â”œâ”€â”€ discovery/      # Protocol discovery components
â”œâ”€â”€ detection/      # Field detection components  
â”œâ”€â”€ anomaly/        # Anomaly detection components
â”œâ”€â”€ features/       # Feature engineering pipeline
â”œâ”€â”€ training/       # Model training infrastructure
â”œâ”€â”€ models/         # Model management and registry
â”œâ”€â”€ api/           # REST and gRPC API implementation
â”œâ”€â”€ monitoring/    # Observability and monitoring
â”œâ”€â”€ tests/         # Comprehensive test suite
â””â”€â”€ deployment/    # Docker and Kubernetes configurations
```

### Contributing Guidelines

1. Fork the repository
2. Create a feature branch
3. Write comprehensive tests
4. Follow code style guidelines (black, flake8)
5. Update documentation
6. Submit pull request with detailed description

## ğŸ§ª Testing

### Running Tests

```bash
# Run all tests
python -m ai_engine.tests.test_runner

# Run specific test categories
python -m ai_engine.tests.test_runner --category unit
python -m ai_engine.tests.test_runner --category api
python -m ai_engine.tests.test_runner --category integration

# Run smoke tests (quick validation)
python -m ai_engine.tests.test_runner --smoke

# Run performance benchmarks
python -m ai_engine.tests.test_runner --benchmark

# Validate installation
python -m ai_engine.tests.test_runner --validate
```

### Test Coverage

The test suite includes:
- **Unit Tests**: Core component functionality
- **Integration Tests**: End-to-end workflows  
- **API Tests**: REST and gRPC endpoint testing
- **Performance Tests**: Load testing and benchmarks
- **Model Tests**: AI model accuracy and performance validation

Target coverage: 90%+ for critical components

## ğŸ“ˆ Performance

### Benchmarks

Typical performance characteristics:

| Operation | Latency (p95) | Throughput | Accuracy |
|-----------|---------------|------------|----------|
| Protocol Discovery | < 150ms | 50 req/s | 92%+ |
| Field Detection | < 120ms | 60 req/s | 89%+ |
| Anomaly Detection | < 180ms | 40 req/s | 85%+ |

### Optimization

1. **Model Optimization**
   - Model quantization for reduced memory usage
   - Batch processing for improved throughput
   - GPU acceleration where available

2. **System Optimization**
   - Async/await patterns for concurrency
   - Connection pooling and caching
   - Resource-aware scaling

## ğŸ” Security

### Security Features

1. **Authentication & Authorization**
   - API key-based authentication
   - JWT tokens for session management
   - Role-based access control (RBAC)

2. **Data Protection**
   - Input validation and sanitization
   - Rate limiting and DDoS protection
   - Secure communication (TLS/SSL)

3. **Privacy & Compliance**
   - Data anonymization capabilities
   - Audit logging and compliance reporting
   - GDPR and privacy-by-design principles

## ğŸ†˜ Troubleshooting

### Common Issues

1. **Model Loading Errors**
   ```bash
   # Check model path and permissions
   ls -la /app/models/
   
   # Verify MLflow connectivity
   curl http://mlflow:5000/health
   ```

2. **High Memory Usage**
   ```bash
   # Monitor memory usage
   kubectl top pods -n qbitel
   
   # Adjust batch size in configuration
   batch_size: 16  # Reduce from default 32
   ```

3. **API Authentication Issues**
   ```bash
   # Get valid API key
   python -c "from ai_engine.api.auth import get_api_key; print(get_api_key())"
   
   # Test authentication
   curl -H "Authorization: Bearer YOUR_API_KEY" http://localhost:8000/status
   ```

### Logs and Debugging

```bash
# View application logs
kubectl logs -n qbitel deployment/qbitel-engine -f

# Enable debug logging
export AI_ENGINE_LOG_LEVEL=DEBUG

# Access distributed traces
# Check monitoring dashboards at http://localhost:3000
```

## ğŸ“ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](../LICENSE) file for details.

## ğŸ¤ Support

- **Documentation**: [docs.qbitel.com](https://docs.qbitel.com)
- **Issues**: [GitHub Issues](https://github.com/yazhsab/ai-engine/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yazhsab/ai-engine/discussions)
- **Enterprise Support**: [enterprise@qbitel.com](mailto:enterprise@qbitel.com)

## ğŸ¯ Roadmap

### Upcoming Features

- [ ] Additional protocol support (MQTT, CoAP, custom protocols)
- [ ] Advanced ensemble methods and AutoML capabilities
- [ ] Real-time streaming processing
- [ ] Enhanced security features and compliance tools
- [ ] Cloud-native scaling and serverless deployment options
- [ ] Advanced visualization and analysis tools

---

**Part of [QBITEL Bridge](https://qbitel.com/bridge)** - Quantum-Safe Legacy Modernization Platform

**Built with â¤ï¸ by the [QBITEL](https://qbitel.com) Team**