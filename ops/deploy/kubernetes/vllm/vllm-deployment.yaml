---
# QBITEL - vLLM Deployment Configuration
# High-performance LLM serving with vLLM
#
# Features:
# - PagedAttention for efficient memory usage
# - Continuous batching for high throughput
# - OpenAI-compatible API
# - GPU support with NVIDIA

apiVersion: v1
kind: Namespace
metadata:
  name: qbitel-vllm
  labels:
    app.kubernetes.io/name: qbitel-vllm
    app.kubernetes.io/part-of: qbitel

---
# ConfigMap for vLLM configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-config
  namespace: qbitel-vllm
data:
  # vLLM server configuration
  VLLM_HOST: "0.0.0.0"
  VLLM_PORT: "8000"

  # Performance tuning
  VLLM_GPU_MEMORY_UTILIZATION: "0.90"
  VLLM_MAX_NUM_SEQS: "256"
  VLLM_MAX_NUM_BATCHED_TOKENS: "32768"
  VLLM_SWAP_SPACE: "4"

  # Logging
  VLLM_LOG_LEVEL: "INFO"

---
# Secret for model access (HuggingFace token)
apiVersion: v1
kind: Secret
metadata:
  name: vllm-secrets
  namespace: qbitel-vllm
type: Opaque
stringData:
  HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"  # Replace with actual token

---
# PersistentVolumeClaim for model cache
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: vllm-model-cache
  namespace: qbitel-vllm
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 500Gi  # Space for multiple models

---
# Llama 3.2 8B Deployment (Fast model)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-8b
  namespace: qbitel-vllm
  labels:
    app: vllm-llama-8b
    model-size: small
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm-llama-8b
  template:
    metadata:
      labels:
        app: vllm-llama-8b
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - containerPort: 8000
              name: http
          args:
            - "--model"
            - "meta-llama/Llama-3.2-8B-Instruct"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--max-num-seqs"
            - "256"
            - "--max-model-len"
            - "8192"
            - "--tensor-parallel-size"
            - "1"
            - "--trust-remote-code"
          envFrom:
            - configMapRef:
                name: vllm-config
            - secretRef:
                name: vllm-secrets
          resources:
            requests:
              memory: "32Gi"
              cpu: "8"
              nvidia.com/gpu: "1"
            limits:
              memory: "48Gi"
              cpu: "16"
              nvidia.com/gpu: "1"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 60
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-model-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi

---
# Service for Llama 8B
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama-8b
  namespace: qbitel-vllm
  labels:
    app: vllm-llama-8b
spec:
  selector:
    app: vllm-llama-8b
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  type: ClusterIP

---
# Llama 3.2 70B Deployment (Powerful model - requires 4 GPUs)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-70b
  namespace: qbitel-vllm
  labels:
    app: vllm-llama-70b
    model-size: large
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama-70b
  template:
    metadata:
      labels:
        app: vllm-llama-70b
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - containerPort: 8000
              name: http
          args:
            - "--model"
            - "meta-llama/Llama-3.2-70B-Instruct"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"
            - "--gpu-memory-utilization"
            - "0.95"
            - "--max-num-seqs"
            - "128"
            - "--max-model-len"
            - "8192"
            - "--tensor-parallel-size"
            - "4"
            - "--trust-remote-code"
          envFrom:
            - configMapRef:
                name: vllm-config
            - secretRef:
                name: vllm-secrets
          resources:
            requests:
              memory: "320Gi"
              cpu: "32"
              nvidia.com/gpu: "4"
            limits:
              memory: "400Gi"
              cpu: "64"
              nvidia.com/gpu: "4"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 180
            periodSeconds: 30
            timeoutSeconds: 15
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-model-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 64Gi

---
# Service for Llama 70B
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama-70b
  namespace: qbitel-vllm
  labels:
    app: vllm-llama-70b
spec:
  selector:
    app: vllm-llama-70b
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  type: ClusterIP

---
# CodeLlama 34B Deployment (Code-focused model)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-codellama
  namespace: qbitel-vllm
  labels:
    app: vllm-codellama
    model-size: medium
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-codellama
  template:
    metadata:
      labels:
        app: vllm-codellama
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-40GB
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.0
          ports:
            - containerPort: 8000
              name: http
          args:
            - "--model"
            - "codellama/CodeLlama-34b-Instruct-hf"
            - "--host"
            - "0.0.0.0"
            - "--port"
            - "8000"
            - "--gpu-memory-utilization"
            - "0.90"
            - "--max-num-seqs"
            - "128"
            - "--max-model-len"
            - "16384"
            - "--tensor-parallel-size"
            - "2"
            - "--trust-remote-code"
          envFrom:
            - configMapRef:
                name: vllm-config
            - secretRef:
                name: vllm-secrets
          resources:
            requests:
              memory: "80Gi"
              cpu: "16"
              nvidia.com/gpu: "2"
            limits:
              memory: "100Gi"
              cpu: "32"
              nvidia.com/gpu: "2"
          volumeMounts:
            - name: model-cache
              mountPath: /root/.cache/huggingface
            - name: shm
              mountPath: /dev/shm
          livenessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 180
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: 8000
            initialDelaySeconds: 120
            periodSeconds: 15
            timeoutSeconds: 5
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-model-cache
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 32Gi

---
# Service for CodeLlama
apiVersion: v1
kind: Service
metadata:
  name: vllm-codellama
  namespace: qbitel-vllm
  labels:
    app: vllm-codellama
spec:
  selector:
    app: vllm-codellama
  ports:
    - port: 8000
      targetPort: 8000
      name: http
  type: ClusterIP

---
# Horizontal Pod Autoscaler for Llama 8B
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-llama-8b-hpa
  namespace: qbitel-vllm
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-llama-8b
  minReplicas: 2
  maxReplicas: 8
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Pods
      pods:
        metric:
          name: vllm_num_requests_running
        target:
          type: AverageValue
          averageValue: "100"

---
# PodDisruptionBudget for Llama 8B
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: vllm-llama-8b-pdb
  namespace: qbitel-vllm
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: vllm-llama-8b

---
# ServiceMonitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: vllm-monitor
  namespace: qbitel-vllm
  labels:
    app: vllm
spec:
  selector:
    matchLabels:
      app: vllm-llama-8b
  endpoints:
    - port: http
      interval: 30s
      path: /metrics

---
# NetworkPolicy for vLLM namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: vllm-network-policy
  namespace: qbitel-vllm
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow from qbitel namespace
    - from:
        - namespaceSelector:
            matchLabels:
              app.kubernetes.io/name: qbitel
      ports:
        - protocol: TCP
          port: 8000
    # Allow from monitoring namespace
    - from:
        - namespaceSelector:
            matchLabels:
              name: monitoring
      ports:
        - protocol: TCP
          port: 8000
  egress:
    # Allow DNS
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: UDP
          port: 53
    # Allow HuggingFace for model downloads
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
