name: Test Automation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements-dev.txt
          pip install -e .
      
      - name: Run unit tests with coverage
        run: |
          pytest ai_engine/tests/ \
            -v \
            --cov=ai_engine \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term \
            --cov-fail-under=80 \
            -m "not integration and not e2e and not slow"
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
      
      - name: Archive coverage reports
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: htmlcov/

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: timescale/timescaledb:latest-pg15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: cronos_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements-dev.txt
          pip install -e .

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:testpass@localhost:5432/cronos_test
          REDIS_URL: redis://localhost:6379
        run: |
          pytest ai_engine/tests/ \
            -v \
            -m "integration" \
            --tb=short
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: test-results/

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: integration-tests

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements-dev.txt
          pip install -e .

      - name: Start application
        run: |
          python -m ai_engine.api.server &
          sleep 10
        env:
          ENVIRONMENT: testing
      
      - name: Run E2E tests
        run: |
          pytest ai_engine/tests/ \
            -v \
            -m "e2e" \
            --tb=short
      
      - name: Upload E2E test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: e2e-test-results
          path: test-results/

  load-tests:
    name: Load Tests
    runs-on: ubuntu-latest
    needs: e2e-tests
    if: github.event_name == 'schedule' || github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements.txt
          pip install --extra-index-url https://download.pytorch.org/whl/cpu -r requirements-dev.txt
          pip install -e .

      - name: Start application
        run: |
          python -m ai_engine.api.server &
          sleep 10
        env:
          ENVIRONMENT: testing
      
      - name: Install k6
        run: |
          sudo gpg -k
          sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
          echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
          sudo apt-get update
          sudo apt-get install k6
      
      - name: Run load tests
        run: |
          k6 run tests/load/k6-load-test.js \
            --out json=load-test-results.json
        env:
          BASE_URL: http://localhost:8000
          API_KEY: cronos_ai_test_key
      
      - name: Upload load test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: load-test-results
          path: load-test-results.json

  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
      
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run Bandit security linter
        run: |
          pip install bandit
          bandit -r ai_engine/ -f json -o bandit-results.json || true
      
      - name: Upload security test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-test-results
          path: |
            trivy-results.sarif
            bandit-results.json

  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install pylint flake8 mypy black isort
      
      - name: Run pylint
        run: |
          pylint ai_engine/ --output-format=json > pylint-results.json || true
      
      - name: Run flake8
        run: |
          flake8 ai_engine/ --format=json --output-file=flake8-results.json || true
      
      - name: Run mypy
        run: |
          mypy ai_engine/ --json-report mypy-results || true
      
      - name: Check code formatting
        run: |
          black --check ai_engine/ || true
          isort --check-only ai_engine/ || true
      
      - name: Upload code quality results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-results
          path: |
            pylint-results.json
            flake8-results.json
            mypy-results/

  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests, security-tests, code-quality]
    if: always()
    
    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
      
      - name: Generate test summary
        run: |
          echo "# Test Automation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- E2E Tests: ${{ needs.e2e-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Security Tests: ${{ needs.security-tests.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Code Quality: ${{ needs.code-quality.result }}" >> $GITHUB_STEP_SUMMARY
      
      - name: Check if all tests passed
        if: |
          needs.unit-tests.result != 'success' ||
          needs.integration-tests.result != 'success' ||
          needs.e2e-tests.result != 'success'
        run: |
          echo "Some tests failed!"
          exit 1